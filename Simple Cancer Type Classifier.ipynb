{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the Encodings to a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "encodings = pd.read_csv(\"binary_encodings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>DonorIDs</th>\n",
       "      <th>CancerType</th>\n",
       "      <th>MU1899169</th>\n",
       "      <th>MU1957569</th>\n",
       "      <th>MU1957631</th>\n",
       "      <th>MU1957694</th>\n",
       "      <th>MU1957756</th>\n",
       "      <th>MU1957895</th>\n",
       "      <th>MU1957974</th>\n",
       "      <th>...</th>\n",
       "      <th>MU130696800</th>\n",
       "      <th>MU122201</th>\n",
       "      <th>MU129795540</th>\n",
       "      <th>MU129540995</th>\n",
       "      <th>MU4885648</th>\n",
       "      <th>MU4468</th>\n",
       "      <th>MU866</th>\n",
       "      <th>MU62030</th>\n",
       "      <th>MU131898417</th>\n",
       "      <th>MU131867962</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>DO48566BLCA-US</td>\n",
       "      <td>BLCA-US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>DO223588BLCA-US</td>\n",
       "      <td>BLCA-US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>DO51948BLCA-US</td>\n",
       "      <td>BLCA-US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>DO514BLCA-US</td>\n",
       "      <td>BLCA-US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>DO474BLCA-US</td>\n",
       "      <td>BLCA-US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>DO481BLCA-US</td>\n",
       "      <td>BLCA-US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>DO48576BLCA-US</td>\n",
       "      <td>BLCA-US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>DO219068BLCA-US</td>\n",
       "      <td>BLCA-US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>DO51906BLCA-US</td>\n",
       "      <td>BLCA-US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>DO48611BLCA-US</td>\n",
       "      <td>BLCA-US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>DO51947BLCA-US</td>\n",
       "      <td>BLCA-US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>410</td>\n",
       "      <td>DO1285BRCA-US</td>\n",
       "      <td>BRCA-US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>411</td>\n",
       "      <td>DO4749BRCA-US</td>\n",
       "      <td>BRCA-US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>412</td>\n",
       "      <td>DO2341BRCA-US</td>\n",
       "      <td>BRCA-US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>413</td>\n",
       "      <td>DO5326BRCA-US</td>\n",
       "      <td>BRCA-US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>414</td>\n",
       "      <td>DO2819BRCA-US</td>\n",
       "      <td>BRCA-US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>415</td>\n",
       "      <td>DO2216BRCA-US</td>\n",
       "      <td>BRCA-US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>416</td>\n",
       "      <td>DO5465BRCA-US</td>\n",
       "      <td>BRCA-US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>417</td>\n",
       "      <td>DO3095BRCA-US</td>\n",
       "      <td>BRCA-US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>418</td>\n",
       "      <td>DO1269BRCA-US</td>\n",
       "      <td>BRCA-US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows Ã— 20267 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0         DonorIDs CancerType  MU1899169  MU1957569  MU1957631  \\\n",
       "0            0   DO48566BLCA-US    BLCA-US        0.0        0.0        0.0   \n",
       "1            1  DO223588BLCA-US    BLCA-US        0.0        0.0        0.0   \n",
       "2            2   DO51948BLCA-US    BLCA-US        0.0        0.0        0.0   \n",
       "3            3     DO514BLCA-US    BLCA-US        0.0        0.0        0.0   \n",
       "4            4     DO474BLCA-US    BLCA-US        0.0        0.0        0.0   \n",
       "5            5     DO481BLCA-US    BLCA-US        0.0        0.0        0.0   \n",
       "6            6   DO48576BLCA-US    BLCA-US        0.0        0.0        0.0   \n",
       "7            7  DO219068BLCA-US    BLCA-US        0.0        0.0        0.0   \n",
       "8            8   DO51906BLCA-US    BLCA-US        0.0        0.0        0.0   \n",
       "9            9   DO48611BLCA-US    BLCA-US        0.0        0.0        0.0   \n",
       "10          10   DO51947BLCA-US    BLCA-US        0.0        0.0        0.0   \n",
       "11         410    DO1285BRCA-US    BRCA-US        0.0        0.0        0.0   \n",
       "12         411    DO4749BRCA-US    BRCA-US        0.0        0.0        0.0   \n",
       "13         412    DO2341BRCA-US    BRCA-US        0.0        0.0        0.0   \n",
       "14         413    DO5326BRCA-US    BRCA-US        0.0        0.0        0.0   \n",
       "15         414    DO2819BRCA-US    BRCA-US        0.0        0.0        0.0   \n",
       "16         415    DO2216BRCA-US    BRCA-US        0.0        0.0        0.0   \n",
       "17         416    DO5465BRCA-US    BRCA-US        0.0        0.0        0.0   \n",
       "18         417    DO3095BRCA-US    BRCA-US        0.0        0.0        0.0   \n",
       "19         418    DO1269BRCA-US    BRCA-US        0.0        0.0        0.0   \n",
       "\n",
       "    MU1957694  MU1957756  MU1957895  MU1957974  ...  MU130696800  MU122201  \\\n",
       "0         0.0        0.0        0.0        0.0  ...          0.0       0.0   \n",
       "1         0.0        0.0        0.0        0.0  ...          0.0       0.0   \n",
       "2         0.0        0.0        0.0        0.0  ...          0.0       0.0   \n",
       "3         0.0        0.0        0.0        0.0  ...          0.0       0.0   \n",
       "4         0.0        0.0        0.0        0.0  ...          0.0       0.0   \n",
       "5         0.0        0.0        0.0        0.0  ...          0.0       0.0   \n",
       "6         0.0        0.0        0.0        0.0  ...          0.0       0.0   \n",
       "7         0.0        0.0        0.0        0.0  ...          0.0       0.0   \n",
       "8         0.0        0.0        0.0        0.0  ...          0.0       0.0   \n",
       "9         0.0        0.0        0.0        0.0  ...          0.0       0.0   \n",
       "10        0.0        0.0        0.0        0.0  ...          0.0       0.0   \n",
       "11        0.0        0.0        0.0        0.0  ...          0.0       0.0   \n",
       "12        0.0        0.0        0.0        0.0  ...          0.0       0.0   \n",
       "13        0.0        0.0        0.0        0.0  ...          0.0       0.0   \n",
       "14        0.0        0.0        0.0        0.0  ...          0.0       0.0   \n",
       "15        0.0        0.0        0.0        0.0  ...          0.0       0.0   \n",
       "16        0.0        0.0        0.0        0.0  ...          0.0       0.0   \n",
       "17        0.0        0.0        0.0        0.0  ...          0.0       0.0   \n",
       "18        0.0        0.0        0.0        0.0  ...          0.0       0.0   \n",
       "19        0.0        0.0        0.0        0.0  ...          0.0       0.0   \n",
       "\n",
       "    MU129795540  MU129540995  MU4885648  MU4468  MU866  MU62030  MU131898417  \\\n",
       "0           0.0          0.0        0.0     0.0    0.0      0.0          0.0   \n",
       "1           0.0          0.0        0.0     0.0    0.0      0.0          0.0   \n",
       "2           0.0          0.0        0.0     0.0    0.0      0.0          0.0   \n",
       "3           0.0          0.0        0.0     0.0    0.0      0.0          0.0   \n",
       "4           0.0          0.0        0.0     0.0    0.0      0.0          0.0   \n",
       "5           0.0          0.0        0.0     0.0    0.0      0.0          0.0   \n",
       "6           0.0          0.0        0.0     0.0    0.0      0.0          0.0   \n",
       "7           0.0          0.0        0.0     0.0    0.0      0.0          0.0   \n",
       "8           0.0          0.0        0.0     0.0    0.0      0.0          0.0   \n",
       "9           0.0          0.0        0.0     0.0    0.0      0.0          0.0   \n",
       "10          0.0          0.0        0.0     0.0    0.0      0.0          1.0   \n",
       "11          0.0          0.0        0.0     0.0    0.0      0.0          0.0   \n",
       "12          0.0          0.0        0.0     0.0    0.0      0.0          0.0   \n",
       "13          0.0          0.0        0.0     0.0    0.0      0.0          0.0   \n",
       "14          0.0          0.0        0.0     0.0    0.0      0.0          0.0   \n",
       "15          0.0          0.0        0.0     0.0    0.0      0.0          0.0   \n",
       "16          0.0          0.0        0.0     1.0    0.0      0.0          0.0   \n",
       "17          0.0          0.0        0.0     1.0    0.0      0.0          0.0   \n",
       "18          0.0          0.0        0.0     0.0    0.0      0.0          0.0   \n",
       "19          0.0          0.0        0.0     0.0    0.0      0.0          0.0   \n",
       "\n",
       "    MU131867962  \n",
       "0           0.0  \n",
       "1           0.0  \n",
       "2           0.0  \n",
       "3           0.0  \n",
       "4           0.0  \n",
       "5           0.0  \n",
       "6           0.0  \n",
       "7           0.0  \n",
       "8           0.0  \n",
       "9           0.0  \n",
       "10          1.0  \n",
       "11          0.0  \n",
       "12          0.0  \n",
       "13          0.0  \n",
       "14          0.0  \n",
       "15          0.0  \n",
       "16          0.0  \n",
       "17          0.0  \n",
       "18          0.0  \n",
       "19          0.0  \n",
       "\n",
       "[20 rows x 20267 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct a Dictionary for Mapping CancerTypes to Integer Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_types = set(list(encodings['CancerType']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BLCA-US',\n",
       " 'BRCA-US',\n",
       " 'COAD-US',\n",
       " 'GBM-US',\n",
       " 'KIRC-US',\n",
       " 'LGG-US',\n",
       " 'LUSC-US',\n",
       " 'OV-US',\n",
       " 'PRAD-US',\n",
       " 'SKCM-US',\n",
       " 'THCA-US',\n",
       " 'UCEC-US'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'KIRC-US': 0,\n",
       " 'BRCA-US': 1,\n",
       " 'PRAD-US': 2,\n",
       " 'BLCA-US': 3,\n",
       " 'COAD-US': 4,\n",
       " 'LGG-US': 5,\n",
       " 'THCA-US': 6,\n",
       " 'SKCM-US': 7,\n",
       " 'LUSC-US': 8,\n",
       " 'GBM-US': 9,\n",
       " 'UCEC-US': 10,\n",
       " 'OV-US': 11}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict = {}\n",
    "for i, ct in enumerate(cancer_types):\n",
    "    label_dict[ct] = i\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutation_labels = [label for label in list(encodings.columns) if label.startswith(\"MU\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutations = encodings[mutation_labels].to_numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20264"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mutations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Function for Converting Labels to One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def to_onehot(label, nclass=12):\n",
    "    result = np.zeros(nclass)\n",
    "    result[label] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct a Labelled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [to_onehot(label_dict[cancer_type]) for cancer_type in list(encodings[\"CancerType\"])]\n",
    "data = list(zip(mutations, label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a Simple Dense Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 20264)             0         \n",
      "_________________________________________________________________\n",
      "Dense1 (Dense)               (None, 500)               10132500  \n",
      "_________________________________________________________________\n",
      "Activation1 (Activation)     (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "Dense2 (Dense)               (None, 50)                25050     \n",
      "_________________________________________________________________\n",
      "Activation2 (Activation)     (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 12)                612       \n",
      "=================================================================\n",
      "Total params: 10,158,162\n",
      "Trainable params: 10,158,162\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Activation, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "X = Input((20264,))\n",
    "H1 = Dense(500, name=\"Dense1\")(X)\n",
    "Z1 = Activation(\"relu\", name=\"Activation1\")(H1)\n",
    "H2 = Dense(50, name=\"Dense2\")(Z1)\n",
    "Z2 = Activation(\"relu\", name=\"Activation2\")(H2)\n",
    "Y = Dense(12, activation=\"softmax\")(Z2)\n",
    "\n",
    "classifier = Model(X, Y)\n",
    "classifier.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate Data into Training and Test Sets and Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data[:500]\n",
    "train = data[500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2864, 20264)\n",
      "(2864, 12)\n"
     ]
    }
   ],
   "source": [
    "datax = np.array([datum for (datum, _) in train])\n",
    "datay = np.array([label for (_, label) in train])\n",
    "print(datax.shape)\n",
    "print(datay.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2291 samples, validate on 573 samples\n",
      "Epoch 1/30\n",
      "2291/2291 [==============================] - 1s 494us/sample - loss: 0.6430 - acc: 0.7905 - val_loss: 1.2721 - val_acc: 0.5881\n",
      "Epoch 2/30\n",
      "2291/2291 [==============================] - 1s 405us/sample - loss: 0.6399 - acc: 0.7918 - val_loss: 1.2744 - val_acc: 0.5881\n",
      "Epoch 3/30\n",
      "2291/2291 [==============================] - 1s 402us/sample - loss: 0.6368 - acc: 0.7927 - val_loss: 1.2740 - val_acc: 0.5881\n",
      "Epoch 4/30\n",
      "2291/2291 [==============================] - 1s 405us/sample - loss: 0.6339 - acc: 0.7927 - val_loss: 1.2751 - val_acc: 0.5881\n",
      "Epoch 5/30\n",
      "2291/2291 [==============================] - 1s 394us/sample - loss: 0.6314 - acc: 0.7931 - val_loss: 1.2772 - val_acc: 0.5881\n",
      "Epoch 6/30\n",
      "2291/2291 [==============================] - 1s 397us/sample - loss: 0.6286 - acc: 0.7931 - val_loss: 1.2790 - val_acc: 0.5881\n",
      "Epoch 7/30\n",
      "2291/2291 [==============================] - 1s 404us/sample - loss: 0.6256 - acc: 0.7940 - val_loss: 1.2802 - val_acc: 0.5846\n",
      "Epoch 8/30\n",
      "2291/2291 [==============================] - 1s 402us/sample - loss: 0.6231 - acc: 0.7935 - val_loss: 1.2826 - val_acc: 0.5881\n",
      "Epoch 9/30\n",
      "2291/2291 [==============================] - 1s 402us/sample - loss: 0.6203 - acc: 0.7940 - val_loss: 1.2821 - val_acc: 0.5881\n",
      "Epoch 10/30\n",
      "2291/2291 [==============================] - 1s 407us/sample - loss: 0.6179 - acc: 0.7935 - val_loss: 1.2834 - val_acc: 0.5864\n",
      "Epoch 11/30\n",
      "2291/2291 [==============================] - 1s 393us/sample - loss: 0.6156 - acc: 0.7935 - val_loss: 1.2858 - val_acc: 0.5864\n",
      "Epoch 12/30\n",
      "2291/2291 [==============================] - 1s 409us/sample - loss: 0.6132 - acc: 0.7940 - val_loss: 1.2870 - val_acc: 0.5864\n",
      "Epoch 13/30\n",
      "2291/2291 [==============================] - 1s 392us/sample - loss: 0.6110 - acc: 0.7940 - val_loss: 1.2882 - val_acc: 0.5864\n",
      "Epoch 14/30\n",
      "2291/2291 [==============================] - 1s 395us/sample - loss: 0.6089 - acc: 0.7940 - val_loss: 1.2899 - val_acc: 0.5846\n",
      "Epoch 15/30\n",
      "2291/2291 [==============================] - 1s 402us/sample - loss: 0.6067 - acc: 0.7940 - val_loss: 1.2914 - val_acc: 0.5864\n",
      "Epoch 16/30\n",
      "2291/2291 [==============================] - 1s 400us/sample - loss: 0.6049 - acc: 0.7940 - val_loss: 1.2939 - val_acc: 0.5846\n",
      "Epoch 17/30\n",
      "2291/2291 [==============================] - 1s 405us/sample - loss: 0.6026 - acc: 0.7948 - val_loss: 1.2961 - val_acc: 0.5864\n",
      "Epoch 18/30\n",
      "2291/2291 [==============================] - 1s 401us/sample - loss: 0.6008 - acc: 0.7953 - val_loss: 1.2965 - val_acc: 0.5846\n",
      "Epoch 19/30\n",
      "2291/2291 [==============================] - 1s 395us/sample - loss: 0.5990 - acc: 0.7953 - val_loss: 1.2989 - val_acc: 0.5846\n",
      "Epoch 20/30\n",
      "2291/2291 [==============================] - 1s 404us/sample - loss: 0.5972 - acc: 0.7953 - val_loss: 1.2996 - val_acc: 0.5829\n",
      "Epoch 21/30\n",
      "2291/2291 [==============================] - 1s 402us/sample - loss: 0.5954 - acc: 0.7953 - val_loss: 1.3027 - val_acc: 0.5846\n",
      "Epoch 22/30\n",
      "2291/2291 [==============================] - 1s 405us/sample - loss: 0.5937 - acc: 0.7962 - val_loss: 1.3032 - val_acc: 0.5864\n",
      "Epoch 23/30\n",
      "2291/2291 [==============================] - 1s 402us/sample - loss: 0.5919 - acc: 0.7966 - val_loss: 1.3059 - val_acc: 0.5846\n",
      "Epoch 24/30\n",
      "2291/2291 [==============================] - 1s 404us/sample - loss: 0.5904 - acc: 0.7970 - val_loss: 1.3076 - val_acc: 0.5829\n",
      "Epoch 25/30\n",
      "2291/2291 [==============================] - 1s 390us/sample - loss: 0.5888 - acc: 0.7970 - val_loss: 1.3103 - val_acc: 0.5846\n",
      "Epoch 26/30\n",
      "2291/2291 [==============================] - 1s 401us/sample - loss: 0.5873 - acc: 0.7970 - val_loss: 1.3103 - val_acc: 0.5829\n",
      "Epoch 27/30\n",
      "2291/2291 [==============================] - 1s 402us/sample - loss: 0.5859 - acc: 0.7970 - val_loss: 1.3133 - val_acc: 0.5864\n",
      "Epoch 28/30\n",
      "2291/2291 [==============================] - 1s 397us/sample - loss: 0.5845 - acc: 0.7975 - val_loss: 1.3140 - val_acc: 0.5881\n",
      "Epoch 29/30\n",
      "2291/2291 [==============================] - 1s 400us/sample - loss: 0.5831 - acc: 0.7975 - val_loss: 1.3168 - val_acc: 0.5846\n",
      "Epoch 30/30\n",
      "2291/2291 [==============================] - 1s 398us/sample - loss: 0.5815 - acc: 0.7979 - val_loss: 1.3176 - val_acc: 0.5829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f78d43d0050>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.00003), metrics=[\"accuracy\"])\n",
    "classifier.fit(x=datax, y=datay, batch_size=32, epochs=30, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model Fits Poorly, and Isn't Generalizing\n",
    "## This is Likely due to the Sparsity of the Inputs\n",
    "## To Remedy this, Attempt Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3011.0,\n",
       " 2371.0,\n",
       " 2302.0,\n",
       " 1826.0,\n",
       " 1608.0,\n",
       " 1486.0,\n",
       " 1354.0,\n",
       " 1316.0,\n",
       " 1212.0,\n",
       " 1094.0,\n",
       " 1058.0,\n",
       " 912.0,\n",
       " 858.0,\n",
       " 857.0,\n",
       " 847.0,\n",
       " 833.0,\n",
       " 805.0,\n",
       " 797.0,\n",
       " 794.0,\n",
       " 793.0,\n",
       " 781.0,\n",
       " 739.0,\n",
       " 724.0,\n",
       " 707.0,\n",
       " 701.0,\n",
       " 636.0,\n",
       " 616.0,\n",
       " 612.0,\n",
       " 595.0,\n",
       " 507.0,\n",
       " 504.0,\n",
       " 500.0,\n",
       " 493.0,\n",
       " 467.0,\n",
       " 424.0,\n",
       " 405.0,\n",
       " 379.0,\n",
       " 377.0,\n",
       " 373.0,\n",
       " 360.0,\n",
       " 335.0,\n",
       " 334.0,\n",
       " 325.0,\n",
       " 320.0,\n",
       " 315.0,\n",
       " 309.0,\n",
       " 309.0,\n",
       " 301.0,\n",
       " 286.0,\n",
       " 268.0,\n",
       " 255.0,\n",
       " 249.0,\n",
       " 238.0,\n",
       " 232.0,\n",
       " 228.0,\n",
       " 225.0,\n",
       " 222.0,\n",
       " 219.0,\n",
       " 215.0,\n",
       " 201.0,\n",
       " 200.0,\n",
       " 196.0,\n",
       " 193.0,\n",
       " 193.0,\n",
       " 192.0,\n",
       " 192.0,\n",
       " 191.0,\n",
       " 183.0,\n",
       " 180.0,\n",
       " 172.0,\n",
       " 161.0,\n",
       " 150.0,\n",
       " 149.0,\n",
       " 147.0,\n",
       " 133.0,\n",
       " 132.0,\n",
       " 131.0,\n",
       " 127.0,\n",
       " 118.0,\n",
       " 115.0,\n",
       " 115.0,\n",
       " 114.0,\n",
       " 113.0,\n",
       " 112.0,\n",
       " 111.0,\n",
       " 110.0,\n",
       " 99.0,\n",
       " 97.0,\n",
       " 95.0,\n",
       " 88.0,\n",
       " 82.0,\n",
       " 79.0,\n",
       " 79.0,\n",
       " 78.0,\n",
       " 78.0,\n",
       " 78.0,\n",
       " 77.0,\n",
       " 77.0,\n",
       " 76.0,\n",
       " 76.0,\n",
       " 74.0,\n",
       " 72.0,\n",
       " 72.0,\n",
       " 70.0,\n",
       " 69.0,\n",
       " 69.0,\n",
       " 69.0,\n",
       " 68.0,\n",
       " 68.0,\n",
       " 67.0,\n",
       " 67.0,\n",
       " 66.0,\n",
       " 66.0,\n",
       " 66.0,\n",
       " 65.0,\n",
       " 65.0,\n",
       " 65.0,\n",
       " 65.0,\n",
       " 64.0,\n",
       " 64.0,\n",
       " 64.0,\n",
       " 63.0,\n",
       " 63.0,\n",
       " 63.0,\n",
       " 63.0,\n",
       " 63.0,\n",
       " 63.0,\n",
       " 62.0,\n",
       " 61.0,\n",
       " 60.0,\n",
       " 58.0,\n",
       " 58.0,\n",
       " 58.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 56.0,\n",
       " 55.0,\n",
       " 54.0,\n",
       " 54.0,\n",
       " 54.0,\n",
       " 54.0,\n",
       " 52.0,\n",
       " 52.0,\n",
       " 52.0,\n",
       " 52.0,\n",
       " 51.0,\n",
       " 51.0,\n",
       " 51.0,\n",
       " 50.0,\n",
       " 50.0,\n",
       " 50.0,\n",
       " 49.0,\n",
       " 49.0,\n",
       " 47.0,\n",
       " 47.0,\n",
       " 47.0,\n",
       " 47.0,\n",
       " 47.0,\n",
       " 47.0,\n",
       " 46.0,\n",
       " 46.0,\n",
       " 46.0,\n",
       " 45.0,\n",
       " 45.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 43.0,\n",
       " 43.0,\n",
       " 43.0,\n",
       " 43.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 41.0,\n",
       " 41.0,\n",
       " 40.0,\n",
       " 40.0,\n",
       " 40.0,\n",
       " 40.0,\n",
       " 40.0,\n",
       " 39.0,\n",
       " 39.0,\n",
       " 39.0,\n",
       " 38.0,\n",
       " 37.0,\n",
       " 37.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 34.0,\n",
       " 34.0,\n",
       " 34.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 31.0,\n",
       " 31.0,\n",
       " 31.0,\n",
       " 31.0,\n",
       " 30.0,\n",
       " 30.0,\n",
       " 30.0,\n",
       " 30.0,\n",
       " 30.0,\n",
       " 30.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 27.0,\n",
       " 27.0,\n",
       " 27.0,\n",
       " 27.0,\n",
       " 27.0,\n",
       " 27.0,\n",
       " 27.0,\n",
       " 26.0,\n",
       " 26.0,\n",
       " 26.0,\n",
       " 26.0,\n",
       " 26.0,\n",
       " 26.0,\n",
       " 26.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 24.0,\n",
       " 24.0,\n",
       " 24.0,\n",
       " 24.0,\n",
       " 24.0,\n",
       " 24.0,\n",
       " 24.0,\n",
       " 24.0,\n",
       " 24.0,\n",
       " 24.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " ...]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutation_counts = ([sum(mutations) for (mutations,_) in data])\n",
    "mutation_counts.sort(reverse=True)\n",
    "mutation_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2736"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([count for count in mutation_counts if count <= 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = TruncatedSVD(n_components=600, n_iter=20)\n",
    "datax_reduced = reducer.fit_transform(datax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2864, 600)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datax_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.06321907e-16,  7.08110458e-15, -1.37715983e-14, -5.77786077e-15,\n",
       "        8.50694020e-17, -6.34750459e-15,  6.49787539e-15, -5.18748666e-16,\n",
       "        7.10284538e-16, -3.63236293e-15, -3.58220698e-17, -4.67911555e-15,\n",
       "        2.91682588e-15, -8.68601461e-15, -7.80696517e-16, -2.16019127e-15,\n",
       "        3.04772482e-16, -3.75592762e-15, -6.98088233e-16,  6.21993337e-15,\n",
       "        1.23318062e-15,  3.40893183e-15, -2.99815846e-15, -2.23574201e-15,\n",
       "        1.09566995e-15, -1.10762702e-15,  1.15703131e-15,  8.34078571e-17,\n",
       "       -1.84128801e-15,  1.25641925e-15, -4.95288763e-17, -7.35997545e-16,\n",
       "        2.53295538e-16,  4.18781484e-16, -1.06461956e-16, -3.79948705e-16,\n",
       "       -8.39293917e-17, -3.84444637e-16,  1.08984548e-16,  1.42754163e-16,\n",
       "       -6.25866551e-17,  1.18342096e-17, -2.65929669e-16, -1.55000128e-16,\n",
       "        1.72385067e-17,  8.71517736e-17, -7.90493852e-17,  2.95088835e-17,\n",
       "       -7.04557606e-18,  1.61771629e-18, -1.47622129e-16, -7.12296566e-17,\n",
       "        8.66383020e-17, -1.13772676e-16,  9.21831519e-17,  1.36635674e-16,\n",
       "        1.25566227e-16, -3.24894660e-17,  8.60108486e-17, -7.02516744e-17,\n",
       "        1.28065465e-16, -9.10170549e-17, -8.34319296e-18, -2.36819021e-17,\n",
       "       -5.28163402e-17, -1.00652100e-17,  2.83500603e-17, -7.21396510e-17,\n",
       "       -8.66746437e-17, -1.95105241e-17, -1.00949007e-16, -3.11274597e-18,\n",
       "       -4.21982623e-19,  6.15387746e-18, -5.26812964e-18, -7.45156887e-17,\n",
       "       -6.21329776e-22, -1.92836424e-17,  9.63064508e-18, -1.69017770e-17,\n",
       "        2.53286515e-17, -7.92122903e-18, -1.97611275e-17,  1.26646439e-17,\n",
       "        1.99488209e-17, -3.10025941e-17,  1.13793174e-17, -1.96863141e-17,\n",
       "        3.04983903e-19,  2.17396018e-17,  1.83750404e-17,  7.17832648e-17,\n",
       "        9.96888416e-17, -5.82313435e-18,  1.75800728e-17, -2.48847875e-17,\n",
       "        1.55999390e-17,  3.69409854e-17, -1.59862364e-17, -1.02306188e-17,\n",
       "       -5.38716530e-17,  3.05205283e-17,  1.16624342e-17, -2.19225750e-17,\n",
       "        2.50175177e-17,  9.77409704e-18,  2.49582507e-17, -3.22711699e-19,\n",
       "        1.45206206e-18, -1.28187128e-17,  3.33630792e-17,  1.01789841e-17,\n",
       "       -1.69253808e-18,  2.74929943e-17, -3.90425431e-17, -9.71377207e-18,\n",
       "        4.94651370e-19, -2.60439805e-18,  3.27407538e-17,  5.67079258e-18,\n",
       "       -1.58314439e-17, -4.07837470e-19,  4.96915920e-18,  2.51443805e-17,\n",
       "       -7.80215903e-19,  1.02933601e-17, -4.72211341e-17, -4.40330876e-17,\n",
       "        6.88035674e-17,  2.51747239e-17,  1.33080281e-17, -3.73150042e-17,\n",
       "        1.13365524e-16,  8.47647592e-17, -5.71786952e-17, -4.89772031e-17,\n",
       "       -1.95455776e-16,  8.26283494e-17, -8.62984702e-17, -1.39042816e-16,\n",
       "       -1.27390974e-16,  1.98761947e-18, -1.47885419e-17, -6.54723676e-17,\n",
       "        1.43446310e-18, -1.29631123e-17,  3.88058575e-17, -7.94768749e-17,\n",
       "        2.16818325e-17,  1.34065040e-17, -1.96808911e-17, -2.50881439e-17,\n",
       "        4.24917531e-17, -2.82222672e-17,  8.38060107e-18, -8.50227023e-18,\n",
       "       -5.85679642e-17,  4.46525053e-17,  4.64350345e-17,  9.45217269e-18,\n",
       "        3.54508455e-17, -4.91299794e-17, -5.30318487e-17, -8.83664215e-17,\n",
       "        4.43056400e-17,  1.61759269e-17, -2.37486727e-17,  5.87366304e-17,\n",
       "       -3.35346357e-17, -2.53411336e-17,  1.76589658e-17,  6.61092362e-17,\n",
       "       -3.87464280e-17,  3.31615462e-17, -9.11052011e-19,  1.78490379e-18,\n",
       "       -3.50944837e-17, -1.39354735e-17,  2.06385499e-17, -5.52894349e-17,\n",
       "       -2.10137432e-17, -2.65292405e-17,  5.18673986e-17, -8.14098323e-17,\n",
       "       -3.29240484e-17,  8.62637926e-17, -8.23614523e-17, -1.42352166e-17,\n",
       "       -4.82527019e-17, -2.66594187e-18,  4.53405011e-17, -2.84388117e-17,\n",
       "       -3.18256590e-17, -5.03178301e-17, -9.17747086e-18, -7.00810521e-18,\n",
       "       -6.78792028e-18, -1.23152680e-17,  4.02308815e-18, -5.98363602e-17,\n",
       "        3.81434910e-17, -3.21133739e-17,  2.93080488e-17, -2.17632112e-17,\n",
       "        9.12973994e-18,  7.39453567e-18, -2.86523648e-17,  4.35420883e-18,\n",
       "        1.39360078e-17, -2.92808300e-17,  1.35431287e-17, -2.32538588e-17,\n",
       "        2.34542726e-17, -7.56927928e-17, -3.57660880e-17, -6.46576227e-17,\n",
       "        2.38750515e-17, -2.07134533e-17,  3.89922222e-17, -4.25181614e-17,\n",
       "        1.52022173e-16, -1.10075583e-16, -6.64770318e-17, -1.08858464e-16,\n",
       "       -1.30666680e-17, -3.85019270e-17,  9.50423311e-17,  8.74637637e-17,\n",
       "       -1.08946788e-16,  1.18090984e-16, -1.27116728e-17,  1.01218541e-16,\n",
       "       -1.69916889e-16,  8.73563585e-17, -1.04543640e-17,  1.31622091e-17,\n",
       "       -8.09835115e-17,  3.13657334e-18,  7.32291113e-17, -1.26676825e-17,\n",
       "       -7.50873092e-17,  5.17289957e-18,  4.38498485e-18, -2.73712534e-17,\n",
       "       -9.82568405e-17, -2.07511607e-17,  1.63225793e-16, -6.56060534e-17,\n",
       "        3.26221497e-17,  2.24105108e-16, -8.04817238e-17, -2.87367235e-17,\n",
       "       -8.23804478e-18,  1.33150956e-16,  1.17813440e-16, -1.16088229e-16,\n",
       "       -1.10122353e-16, -9.27695899e-18, -4.26479432e-17, -8.75082654e-17,\n",
       "        4.91016600e-17, -9.13424979e-18, -1.38920486e-17, -2.95533992e-17,\n",
       "       -3.25268249e-17, -2.77419329e-18, -2.53979459e-17,  1.36024208e-17,\n",
       "        3.24574966e-17, -1.12437965e-16, -2.33436268e-17,  2.96472910e-17,\n",
       "        6.40549008e-17, -2.10462312e-17, -3.51006983e-17,  4.99736273e-17,\n",
       "        7.08616831e-18,  3.41726709e-17,  1.18802433e-18,  3.29868540e-17,\n",
       "       -6.52636071e-17,  4.36335653e-18,  9.74837826e-18,  3.58691352e-17,\n",
       "        1.93718240e-17,  3.16863099e-17,  1.73129784e-17,  3.35278305e-17,\n",
       "        7.43562148e-17, -5.99427781e-17, -3.43658623e-17, -3.41992588e-17,\n",
       "       -7.69648694e-17, -3.49759827e-17,  7.19547707e-17, -5.48286600e-17,\n",
       "       -1.12295753e-17, -2.92742601e-17,  2.07262263e-17, -6.29025498e-17,\n",
       "       -3.00350612e-17, -8.00241284e-17,  5.73346043e-17, -5.58147325e-17,\n",
       "        1.25270821e-16,  4.74810516e-17,  1.59282238e-17,  1.38851413e-17,\n",
       "        3.16476152e-17,  8.44495827e-17, -1.92654661e-17, -5.83940767e-17,\n",
       "        1.09134273e-16,  4.09180058e-17, -7.62187594e-18, -1.12034295e-17,\n",
       "        7.00903424e-17,  2.26218197e-17, -8.22650875e-17, -9.59635033e-18,\n",
       "       -1.53506477e-17, -3.62138519e-17,  1.05348154e-16, -2.85520356e-17,\n",
       "        1.39329878e-16,  6.63063212e-17, -2.13741794e-17,  6.83830905e-17,\n",
       "        6.47408937e-17, -1.19636663e-16,  7.93713579e-17, -4.91468857e-17,\n",
       "        3.74231847e-17,  2.23781739e-17,  4.65544381e-17, -6.55382837e-17,\n",
       "       -3.59747790e-18,  5.32322257e-17,  1.93365540e-17,  1.59178999e-17,\n",
       "        6.44423948e-17, -7.12205343e-17, -4.52607104e-17,  5.45142010e-17,\n",
       "       -6.20426660e-17,  3.40911688e-17,  4.47713162e-17,  8.41931002e-18,\n",
       "        3.10450714e-17, -4.06087907e-17,  1.20462961e-18, -2.66112422e-17,\n",
       "        3.59701159e-18,  1.53921136e-18, -2.69785244e-17,  4.98558437e-17,\n",
       "       -3.13725965e-17,  9.91531364e-17, -3.37999641e-17,  3.34612236e-18,\n",
       "        4.85332313e-17, -7.30229957e-18, -1.29275514e-17, -3.64589306e-17,\n",
       "       -3.20193103e-17,  4.68881782e-17, -2.93868061e-17,  7.29536115e-19,\n",
       "       -6.72230664e-19,  8.77188957e-17,  5.62902943e-17, -6.75237168e-18,\n",
       "       -2.21531129e-17,  4.61966579e-17,  2.93164507e-17, -4.97168929e-17,\n",
       "       -1.04878909e-17, -1.41168180e-18,  1.04243826e-16,  2.12210754e-17,\n",
       "        2.83523025e-17,  4.64540734e-17,  6.10413493e-17, -4.78769100e-17,\n",
       "       -6.04547641e-17,  3.35945648e-17, -4.87293589e-17, -6.23956246e-17,\n",
       "        1.14910018e-17,  5.33158059e-17,  3.84465453e-17, -6.08678232e-17,\n",
       "        1.79492288e-17, -1.34655103e-17, -5.87593706e-17, -7.62240543e-17,\n",
       "        8.60228012e-17,  9.50934335e-18,  4.23613098e-18,  1.41666889e-17,\n",
       "        8.54161654e-17, -1.60322024e-18,  7.08276264e-17, -3.42167967e-17,\n",
       "        6.49484827e-17, -6.93045715e-17,  3.70386625e-17, -5.39626131e-17,\n",
       "       -5.02245882e-17,  3.11253159e-17,  2.31963393e-17, -6.17135074e-17,\n",
       "       -4.71741434e-17,  5.58065330e-17,  2.33587338e-17, -2.70381230e-17,\n",
       "        2.73968951e-17, -3.57830042e-17,  3.81136543e-17,  7.61071907e-18,\n",
       "       -1.14497189e-17,  1.40514164e-19,  4.18029183e-17, -4.77725407e-17,\n",
       "       -2.21782914e-17,  1.72755827e-17,  1.40642745e-17,  9.59384252e-19,\n",
       "       -5.48780412e-17,  6.48361156e-18,  2.58242062e-17,  3.73954477e-17,\n",
       "        1.82660776e-17, -2.49104205e-17,  4.40561394e-17,  5.87250044e-17,\n",
       "       -3.35821356e-17,  3.13291970e-17,  1.52943144e-17, -3.73305135e-17,\n",
       "        6.95202534e-18,  4.43751121e-18, -2.09688145e-17, -2.61940578e-17,\n",
       "        5.59579349e-18,  4.71491414e-17, -3.30193871e-18,  2.27601611e-17,\n",
       "        7.52937339e-18, -2.74579623e-17, -2.72692659e-17,  1.02106941e-17,\n",
       "        2.67839844e-18, -2.35123664e-17, -1.49369881e-17, -2.37912531e-17,\n",
       "       -2.44527312e-17,  2.95002244e-18,  1.16767549e-17,  2.42420308e-19,\n",
       "        2.73470891e-17,  4.14659880e-17, -1.28407974e-17, -2.38019032e-17,\n",
       "       -4.11826199e-18,  3.45434001e-17, -4.85464817e-17,  3.83165652e-17,\n",
       "       -3.95599767e-17, -4.50870082e-17,  5.85854718e-17,  2.31310187e-17,\n",
       "       -1.32231577e-18, -1.61021073e-17,  2.48181551e-17,  1.70754860e-17,\n",
       "        1.84685835e-17,  4.23333662e-18, -2.34636114e-17,  3.19999449e-19,\n",
       "       -1.73482148e-18, -3.12289556e-17, -1.55395150e-17,  6.94264070e-17,\n",
       "        2.04452416e-17, -2.47265302e-17,  2.83948677e-17,  4.15273705e-18,\n",
       "        3.01933485e-17, -6.02224379e-18, -1.28453857e-17,  9.64042231e-18,\n",
       "       -2.40498151e-17,  3.71335092e-17,  1.22650230e-17,  2.58611445e-17,\n",
       "       -2.52588847e-17, -6.66172892e-17, -9.82353953e-17,  4.67894549e-17,\n",
       "        2.98084682e-17, -2.34190616e-17,  1.51659427e-17,  7.80141912e-18,\n",
       "       -1.34322077e-17,  1.78450738e-17, -3.17399504e-17,  1.38608923e-17,\n",
       "       -7.40639608e-18, -7.41863104e-18,  2.39398154e-17, -5.63201842e-18,\n",
       "       -1.38602310e-17, -1.33219976e-18, -2.62746106e-17,  2.62910395e-17,\n",
       "        1.88892861e-17, -1.37141277e-17,  4.16411451e-17,  2.27576930e-17,\n",
       "       -3.00321363e-17,  9.74578491e-18, -5.75200925e-18, -6.35923132e-17,\n",
       "        1.23473138e-17, -2.22603973e-17, -3.95289387e-17,  2.00581984e-17,\n",
       "        1.99630693e-18,  2.06581993e-17, -1.93149387e-17,  6.14277183e-17,\n",
       "       -6.07653769e-17,  1.54384038e-18,  3.19902888e-17, -9.79246934e-18,\n",
       "       -1.61505076e-17,  1.01028170e-17, -2.27141609e-17, -2.42997302e-17,\n",
       "        5.29867084e-17, -1.44999370e-17,  1.54448741e-18,  3.17841609e-18,\n",
       "        2.07612914e-17, -3.29444175e-17, -1.04296594e-17, -2.21883840e-17,\n",
       "       -3.09920853e-18, -1.50595144e-18,  1.77241187e-17,  3.26042278e-17,\n",
       "       -1.67033919e-17, -9.54178994e-18, -9.23295193e-18, -1.62122003e-18,\n",
       "       -1.14311283e-17,  1.36999060e-17,  1.44251991e-18,  7.63702182e-18,\n",
       "       -1.72871147e-17, -1.53614965e-17,  1.67743704e-17, -4.16727189e-17,\n",
       "        9.74222270e-18,  2.20554362e-17,  1.15904157e-17, -5.67364783e-17,\n",
       "        3.42241291e-18, -1.43178121e-17,  2.25465156e-17,  5.18951813e-17,\n",
       "        1.11090935e-17, -6.46723241e-17,  1.13351689e-18, -1.06545103e-17,\n",
       "        4.27665838e-18, -1.28172756e-17,  1.38539033e-18, -4.82099801e-17,\n",
       "        2.64289454e-17, -6.10557408e-18, -1.82741727e-17, -8.78879021e-18,\n",
       "       -2.78806125e-17, -6.16360337e-18, -3.27504888e-17, -7.72682483e-18,\n",
       "       -3.34799701e-17,  9.97743072e-18,  1.22017424e-17, -1.80160608e-17,\n",
       "        9.17609119e-18,  1.91345755e-18,  2.93270919e-17,  4.34575309e-18,\n",
       "       -5.97928457e-18,  4.47346348e-17, -2.63040954e-17,  1.42207785e-17])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "Dense1 (Dense)               (None, 300)               180300    \n",
      "_________________________________________________________________\n",
      "Activation1 (Activation)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "Dense2 (Dense)               (None, 50)                15050     \n",
      "_________________________________________________________________\n",
      "Activation2 (Activation)     (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 12)                612       \n",
      "=================================================================\n",
      "Total params: 195,962\n",
      "Trainable params: 195,962\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "X = Input((600,))\n",
    "H1 = Dense(300, name=\"Dense1\")(X)\n",
    "Z1 = Activation(\"relu\", name=\"Activation1\")(H1)\n",
    "H2 = Dense(50, name=\"Dense2\")(Z1)\n",
    "Z2 = Activation(\"relu\", name=\"Activation2\")(H2)\n",
    "Y = Dense(12, activation=\"softmax\")(Z2)\n",
    "\n",
    "classifier2 = Model(X, Y)\n",
    "classifier2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2291 samples, validate on 573 samples\n",
      "Epoch 1/100\n",
      "2291/2291 [==============================] - 1s 284us/sample - loss: 2.4063 - acc: 0.3278 - val_loss: 2.2740 - val_acc: 0.4450\n",
      "Epoch 2/100\n",
      "2291/2291 [==============================] - 0s 150us/sample - loss: 2.1578 - acc: 0.4526 - val_loss: 2.0293 - val_acc: 0.4695\n",
      "Epoch 3/100\n",
      "2291/2291 [==============================] - 0s 153us/sample - loss: 1.8686 - acc: 0.4928 - val_loss: 1.7563 - val_acc: 0.4887\n",
      "Epoch 4/100\n",
      "2291/2291 [==============================] - 0s 149us/sample - loss: 1.6027 - acc: 0.5286 - val_loss: 1.5760 - val_acc: 0.5218\n",
      "Epoch 5/100\n",
      "2291/2291 [==============================] - 0s 148us/sample - loss: 1.4244 - acc: 0.5578 - val_loss: 1.4769 - val_acc: 0.5410\n",
      "Epoch 6/100\n",
      "2291/2291 [==============================] - 0s 153us/sample - loss: 1.3146 - acc: 0.5862 - val_loss: 1.4200 - val_acc: 0.5602\n",
      "Epoch 7/100\n",
      "2291/2291 [==============================] - 0s 147us/sample - loss: 1.2419 - acc: 0.6015 - val_loss: 1.3776 - val_acc: 0.5602\n",
      "Epoch 8/100\n",
      "2291/2291 [==============================] - 0s 155us/sample - loss: 1.1909 - acc: 0.6093 - val_loss: 1.3476 - val_acc: 0.5742\n",
      "Epoch 9/100\n",
      "2291/2291 [==============================] - 0s 152us/sample - loss: 1.1519 - acc: 0.6185 - val_loss: 1.3283 - val_acc: 0.5742\n",
      "Epoch 10/100\n",
      "2291/2291 [==============================] - 0s 156us/sample - loss: 1.1217 - acc: 0.6224 - val_loss: 1.3112 - val_acc: 0.5794\n",
      "Epoch 11/100\n",
      "2291/2291 [==============================] - 0s 154us/sample - loss: 1.0952 - acc: 0.6246 - val_loss: 1.3000 - val_acc: 0.5829\n",
      "Epoch 12/100\n",
      "2291/2291 [==============================] - 0s 150us/sample - loss: 1.0753 - acc: 0.6299 - val_loss: 1.2910 - val_acc: 0.5777\n",
      "Epoch 13/100\n",
      "2291/2291 [==============================] - 0s 146us/sample - loss: 1.0579 - acc: 0.6333 - val_loss: 1.2846 - val_acc: 0.5864\n",
      "Epoch 14/100\n",
      "2291/2291 [==============================] - 0s 154us/sample - loss: 1.0433 - acc: 0.6329 - val_loss: 1.2793 - val_acc: 0.5812\n",
      "Epoch 15/100\n",
      "2291/2291 [==============================] - 0s 154us/sample - loss: 1.0295 - acc: 0.6347 - val_loss: 1.2810 - val_acc: 0.5812\n",
      "Epoch 16/100\n",
      "2291/2291 [==============================] - 0s 150us/sample - loss: 1.0167 - acc: 0.6390 - val_loss: 1.2760 - val_acc: 0.5794\n",
      "Epoch 17/100\n",
      "2291/2291 [==============================] - 0s 148us/sample - loss: 1.0085 - acc: 0.6425 - val_loss: 1.2869 - val_acc: 0.5846\n",
      "Epoch 18/100\n",
      "2291/2291 [==============================] - 0s 151us/sample - loss: 0.9982 - acc: 0.6451 - val_loss: 1.2814 - val_acc: 0.5812\n",
      "Epoch 19/100\n",
      "2291/2291 [==============================] - 0s 150us/sample - loss: 0.9880 - acc: 0.6456 - val_loss: 1.2852 - val_acc: 0.5707\n",
      "Epoch 20/100\n",
      "2291/2291 [==============================] - 0s 153us/sample - loss: 0.9808 - acc: 0.6491 - val_loss: 1.2874 - val_acc: 0.5794\n",
      "Epoch 21/100\n",
      "2291/2291 [==============================] - 0s 149us/sample - loss: 0.9743 - acc: 0.6517 - val_loss: 1.2949 - val_acc: 0.5777\n",
      "Epoch 22/100\n",
      "2291/2291 [==============================] - 0s 146us/sample - loss: 0.9674 - acc: 0.6491 - val_loss: 1.3073 - val_acc: 0.5759\n",
      "Epoch 23/100\n",
      "2291/2291 [==============================] - 0s 150us/sample - loss: 0.9602 - acc: 0.6591 - val_loss: 1.2946 - val_acc: 0.5812\n",
      "Epoch 24/100\n",
      "2291/2291 [==============================] - 0s 151us/sample - loss: 0.9543 - acc: 0.6595 - val_loss: 1.3185 - val_acc: 0.5777\n",
      "Epoch 25/100\n",
      "2291/2291 [==============================] - 0s 154us/sample - loss: 0.9491 - acc: 0.6656 - val_loss: 1.3201 - val_acc: 0.5707\n",
      "Epoch 26/100\n",
      "2291/2291 [==============================] - 0s 151us/sample - loss: 0.9420 - acc: 0.6674 - val_loss: 1.3313 - val_acc: 0.5742\n",
      "Epoch 27/100\n",
      "2291/2291 [==============================] - 0s 151us/sample - loss: 0.9366 - acc: 0.6665 - val_loss: 1.3427 - val_acc: 0.5777\n",
      "Epoch 28/100\n",
      "2291/2291 [==============================] - 0s 153us/sample - loss: 0.9326 - acc: 0.6674 - val_loss: 1.3473 - val_acc: 0.5759\n",
      "Epoch 29/100\n",
      "2291/2291 [==============================] - 0s 152us/sample - loss: 0.9282 - acc: 0.6713 - val_loss: 1.3582 - val_acc: 0.5724\n",
      "Epoch 30/100\n",
      "2291/2291 [==============================] - 0s 147us/sample - loss: 0.9218 - acc: 0.6766 - val_loss: 1.3843 - val_acc: 0.5672\n",
      "Epoch 31/100\n",
      "2291/2291 [==============================] - 0s 149us/sample - loss: 0.9188 - acc: 0.6753 - val_loss: 1.3863 - val_acc: 0.5672\n",
      "Epoch 32/100\n",
      "2291/2291 [==============================] - 0s 149us/sample - loss: 0.9140 - acc: 0.6774 - val_loss: 1.3921 - val_acc: 0.5689\n",
      "Epoch 33/100\n",
      "2291/2291 [==============================] - 0s 149us/sample - loss: 0.9078 - acc: 0.6761 - val_loss: 1.4165 - val_acc: 0.5620\n",
      "Epoch 34/100\n",
      "2291/2291 [==============================] - 0s 152us/sample - loss: 0.9045 - acc: 0.6753 - val_loss: 1.4273 - val_acc: 0.5620\n",
      "Epoch 35/100\n",
      "2291/2291 [==============================] - 0s 151us/sample - loss: 0.9008 - acc: 0.6770 - val_loss: 1.4447 - val_acc: 0.5620\n",
      "Epoch 36/100\n",
      "2291/2291 [==============================] - 0s 148us/sample - loss: 0.8953 - acc: 0.6753 - val_loss: 1.4619 - val_acc: 0.5532\n",
      "Epoch 37/100\n",
      "2291/2291 [==============================] - 0s 153us/sample - loss: 0.8916 - acc: 0.6805 - val_loss: 1.4811 - val_acc: 0.5497\n",
      "Epoch 38/100\n",
      "2291/2291 [==============================] - 0s 154us/sample - loss: 0.8888 - acc: 0.6796 - val_loss: 1.4900 - val_acc: 0.5585\n",
      "Epoch 39/100\n",
      "2291/2291 [==============================] - 0s 152us/sample - loss: 0.8864 - acc: 0.6809 - val_loss: 1.4988 - val_acc: 0.5550\n",
      "Epoch 40/100\n",
      "2291/2291 [==============================] - 0s 154us/sample - loss: 0.8836 - acc: 0.6875 - val_loss: 1.5174 - val_acc: 0.5462\n",
      "Epoch 41/100\n",
      "2291/2291 [==============================] - 0s 151us/sample - loss: 0.8791 - acc: 0.6866 - val_loss: 1.5268 - val_acc: 0.5497\n",
      "Epoch 42/100\n",
      "2291/2291 [==============================] - 0s 148us/sample - loss: 0.8730 - acc: 0.6875 - val_loss: 1.5459 - val_acc: 0.5462\n",
      "Epoch 43/100\n",
      "2291/2291 [==============================] - 0s 150us/sample - loss: 0.8712 - acc: 0.6888 - val_loss: 1.5775 - val_acc: 0.5445\n",
      "Epoch 44/100\n",
      "2291/2291 [==============================] - 0s 156us/sample - loss: 0.8672 - acc: 0.6905 - val_loss: 1.5865 - val_acc: 0.5480\n",
      "Epoch 45/100\n",
      "2291/2291 [==============================] - 0s 150us/sample - loss: 0.8654 - acc: 0.6923 - val_loss: 1.6037 - val_acc: 0.5462\n",
      "Epoch 46/100\n",
      "2291/2291 [==============================] - 0s 151us/sample - loss: 0.8632 - acc: 0.6927 - val_loss: 1.6184 - val_acc: 0.5428\n",
      "Epoch 47/100\n",
      "2291/2291 [==============================] - 0s 153us/sample - loss: 0.8605 - acc: 0.6931 - val_loss: 1.6418 - val_acc: 0.5410\n",
      "Epoch 48/100\n",
      "2291/2291 [==============================] - 0s 150us/sample - loss: 0.8545 - acc: 0.6966 - val_loss: 1.6793 - val_acc: 0.5410\n",
      "Epoch 49/100\n",
      "2291/2291 [==============================] - 0s 152us/sample - loss: 0.8520 - acc: 0.6953 - val_loss: 1.6989 - val_acc: 0.5375\n",
      "Epoch 50/100\n",
      "2291/2291 [==============================] - 0s 158us/sample - loss: 0.8504 - acc: 0.7001 - val_loss: 1.6804 - val_acc: 0.5480\n",
      "Epoch 51/100\n",
      "2291/2291 [==============================] - 0s 151us/sample - loss: 0.8479 - acc: 0.6993 - val_loss: 1.6998 - val_acc: 0.5393\n",
      "Epoch 52/100\n",
      "2291/2291 [==============================] - 0s 152us/sample - loss: 0.8431 - acc: 0.7023 - val_loss: 1.7425 - val_acc: 0.5393\n",
      "Epoch 53/100\n",
      "2291/2291 [==============================] - 0s 149us/sample - loss: 0.8440 - acc: 0.7014 - val_loss: 1.7417 - val_acc: 0.5462\n",
      "Epoch 54/100\n",
      "2291/2291 [==============================] - 0s 152us/sample - loss: 0.8364 - acc: 0.7036 - val_loss: 1.7841 - val_acc: 0.5428\n",
      "Epoch 55/100\n",
      "2291/2291 [==============================] - 0s 151us/sample - loss: 0.8369 - acc: 0.7019 - val_loss: 1.8014 - val_acc: 0.5410\n",
      "Epoch 56/100\n",
      "2291/2291 [==============================] - 0s 151us/sample - loss: 0.8330 - acc: 0.7010 - val_loss: 1.7982 - val_acc: 0.5410\n",
      "Epoch 57/100\n",
      "2291/2291 [==============================] - 0s 148us/sample - loss: 0.8327 - acc: 0.7036 - val_loss: 1.8337 - val_acc: 0.5445\n",
      "Epoch 58/100\n",
      "2291/2291 [==============================] - 0s 152us/sample - loss: 0.8292 - acc: 0.7045 - val_loss: 1.8421 - val_acc: 0.5393\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2291/2291 [==============================] - 0s 151us/sample - loss: 0.8280 - acc: 0.7041 - val_loss: 1.8570 - val_acc: 0.5428\n",
      "Epoch 60/100\n",
      "2291/2291 [==============================] - 0s 151us/sample - loss: 0.8256 - acc: 0.7049 - val_loss: 1.8548 - val_acc: 0.5428\n",
      "Epoch 61/100\n",
      "2291/2291 [==============================] - 0s 152us/sample - loss: 0.8233 - acc: 0.7076 - val_loss: 1.9045 - val_acc: 0.5358\n",
      "Epoch 62/100\n",
      "2291/2291 [==============================] - 0s 151us/sample - loss: 0.8211 - acc: 0.7067 - val_loss: 1.8891 - val_acc: 0.5358\n",
      "Epoch 63/100\n",
      "2291/2291 [==============================] - 0s 150us/sample - loss: 0.8176 - acc: 0.7080 - val_loss: 1.9108 - val_acc: 0.5358\n",
      "Epoch 64/100\n",
      "2291/2291 [==============================] - 0s 156us/sample - loss: 0.8169 - acc: 0.7093 - val_loss: 1.9317 - val_acc: 0.5288\n",
      "Epoch 65/100\n",
      "2291/2291 [==============================] - 0s 150us/sample - loss: 0.8162 - acc: 0.7067 - val_loss: 1.9209 - val_acc: 0.5340\n",
      "Epoch 66/100\n",
      "2291/2291 [==============================] - 0s 153us/sample - loss: 0.8126 - acc: 0.7067 - val_loss: 1.9534 - val_acc: 0.5340\n",
      "Epoch 67/100\n",
      "2291/2291 [==============================] - 0s 154us/sample - loss: 0.8135 - acc: 0.7054 - val_loss: 1.9674 - val_acc: 0.5410\n",
      "Epoch 68/100\n",
      "2291/2291 [==============================] - 0s 153us/sample - loss: 0.8101 - acc: 0.7084 - val_loss: 1.9503 - val_acc: 0.5393\n",
      "Epoch 69/100\n",
      "2291/2291 [==============================] - 0s 153us/sample - loss: 0.8056 - acc: 0.7119 - val_loss: 1.9594 - val_acc: 0.5323\n",
      "Epoch 70/100\n",
      "2291/2291 [==============================] - 0s 154us/sample - loss: 0.8046 - acc: 0.7115 - val_loss: 1.9895 - val_acc: 0.5323\n",
      "Epoch 71/100\n",
      "2291/2291 [==============================] - 0s 153us/sample - loss: 0.8039 - acc: 0.7102 - val_loss: 1.9990 - val_acc: 0.5393\n",
      "Epoch 72/100\n",
      "2291/2291 [==============================] - 0s 154us/sample - loss: 0.8015 - acc: 0.7128 - val_loss: 2.0424 - val_acc: 0.5375\n",
      "Epoch 73/100\n",
      "2291/2291 [==============================] - 0s 150us/sample - loss: 0.8012 - acc: 0.7110 - val_loss: 2.0392 - val_acc: 0.5375\n",
      "Epoch 74/100\n",
      "2291/2291 [==============================] - 0s 150us/sample - loss: 0.7999 - acc: 0.7119 - val_loss: 2.0171 - val_acc: 0.5393\n",
      "Epoch 75/100\n",
      "2291/2291 [==============================] - 0s 145us/sample - loss: 0.8002 - acc: 0.7167 - val_loss: 2.0608 - val_acc: 0.5340\n",
      "Epoch 76/100\n",
      "2291/2291 [==============================] - 0s 152us/sample - loss: 0.7953 - acc: 0.7080 - val_loss: 2.0359 - val_acc: 0.5323\n",
      "Epoch 77/100\n",
      "2291/2291 [==============================] - 0s 148us/sample - loss: 0.7973 - acc: 0.7093 - val_loss: 2.0384 - val_acc: 0.5428\n",
      "Epoch 78/100\n",
      "2291/2291 [==============================] - 0s 149us/sample - loss: 0.7918 - acc: 0.7119 - val_loss: 2.0371 - val_acc: 0.5480\n",
      "Epoch 79/100\n",
      "2291/2291 [==============================] - 0s 156us/sample - loss: 0.7906 - acc: 0.7106 - val_loss: 2.0635 - val_acc: 0.5305\n",
      "Epoch 80/100\n",
      "2291/2291 [==============================] - 0s 155us/sample - loss: 0.7905 - acc: 0.7115 - val_loss: 2.0715 - val_acc: 0.5340\n",
      "Epoch 81/100\n",
      "2291/2291 [==============================] - 0s 151us/sample - loss: 0.7876 - acc: 0.7084 - val_loss: 2.0841 - val_acc: 0.5462\n",
      "Epoch 82/100\n",
      "2291/2291 [==============================] - 0s 149us/sample - loss: 0.7890 - acc: 0.7145 - val_loss: 2.1039 - val_acc: 0.5288\n",
      "Epoch 83/100\n",
      "2291/2291 [==============================] - 0s 154us/sample - loss: 0.7862 - acc: 0.7158 - val_loss: 2.0755 - val_acc: 0.5305\n",
      "Epoch 84/100\n",
      "2291/2291 [==============================] - 0s 152us/sample - loss: 0.7839 - acc: 0.7132 - val_loss: 2.1207 - val_acc: 0.5340\n",
      "Epoch 85/100\n",
      "2291/2291 [==============================] - 0s 153us/sample - loss: 0.7829 - acc: 0.7150 - val_loss: 2.1040 - val_acc: 0.5375\n",
      "Epoch 86/100\n",
      "2291/2291 [==============================] - 0s 149us/sample - loss: 0.7838 - acc: 0.7097 - val_loss: 2.1100 - val_acc: 0.5323\n",
      "Epoch 87/100\n",
      "2291/2291 [==============================] - 0s 151us/sample - loss: 0.7855 - acc: 0.7154 - val_loss: 2.1259 - val_acc: 0.5305\n",
      "Epoch 88/100\n",
      "2291/2291 [==============================] - 0s 152us/sample - loss: 0.7791 - acc: 0.7180 - val_loss: 2.1303 - val_acc: 0.5253\n",
      "Epoch 89/100\n",
      "2291/2291 [==============================] - 0s 156us/sample - loss: 0.7798 - acc: 0.7172 - val_loss: 2.1484 - val_acc: 0.5236\n",
      "Epoch 90/100\n",
      "2291/2291 [==============================] - 0s 150us/sample - loss: 0.7798 - acc: 0.7158 - val_loss: 2.1448 - val_acc: 0.5375\n",
      "Epoch 91/100\n",
      "2291/2291 [==============================] - 0s 149us/sample - loss: 0.7770 - acc: 0.7180 - val_loss: 2.1477 - val_acc: 0.5271\n",
      "Epoch 92/100\n",
      "2291/2291 [==============================] - 0s 151us/sample - loss: 0.7769 - acc: 0.7189 - val_loss: 2.1499 - val_acc: 0.5340\n",
      "Epoch 93/100\n",
      "2291/2291 [==============================] - 0s 154us/sample - loss: 0.7747 - acc: 0.7193 - val_loss: 2.1889 - val_acc: 0.5183\n",
      "Epoch 94/100\n",
      "2291/2291 [==============================] - 0s 147us/sample - loss: 0.7754 - acc: 0.7158 - val_loss: 2.1634 - val_acc: 0.5271\n",
      "Epoch 95/100\n",
      "2291/2291 [==============================] - 0s 152us/sample - loss: 0.7716 - acc: 0.7137 - val_loss: 2.1940 - val_acc: 0.5271\n",
      "Epoch 96/100\n",
      "2291/2291 [==============================] - 0s 154us/sample - loss: 0.7705 - acc: 0.7110 - val_loss: 2.1864 - val_acc: 0.5271\n",
      "Epoch 97/100\n",
      "2291/2291 [==============================] - 0s 155us/sample - loss: 0.7675 - acc: 0.7215 - val_loss: 2.1873 - val_acc: 0.5288\n",
      "Epoch 98/100\n",
      "2291/2291 [==============================] - 0s 150us/sample - loss: 0.7695 - acc: 0.7193 - val_loss: 2.1978 - val_acc: 0.5218\n",
      "Epoch 99/100\n",
      "2291/2291 [==============================] - 0s 149us/sample - loss: 0.7680 - acc: 0.7185 - val_loss: 2.1981 - val_acc: 0.5253\n",
      "Epoch 100/100\n",
      "2291/2291 [==============================] - 0s 153us/sample - loss: 0.7660 - acc: 0.7220 - val_loss: 2.2115 - val_acc: 0.5218\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f765c334e90>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier2.compile(optimizer=Adam(lr=0.0003), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "classifier2.fit(datax_reduced, datay, batch_size=32, epochs=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model Trained on Reduced Inputs is Actually Worse...\n",
    "## We Can Attempt a Different Encoding Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_encoding(mutations, d=3011):\n",
    "    result = []\n",
    "    for j, b in enumerate(mutations):\n",
    "        if b == 1:\n",
    "            result.append(j/len(mutations))\n",
    "    while len(result) < 3011:\n",
    "        result.append(-1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transformed = [(transform_encoding(m),l) for (m,l) in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        (None, 3011)              0         \n",
      "_________________________________________________________________\n",
      "Dense1 (Dense)               (None, 300)               903600    \n",
      "_________________________________________________________________\n",
      "Activation1 (Activation)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "Dense2 (Dense)               (None, 50)                15050     \n",
      "_________________________________________________________________\n",
      "Activation2 (Activation)     (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 12)                612       \n",
      "=================================================================\n",
      "Total params: 919,262\n",
      "Trainable params: 919,262\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "X = Input((3011,))\n",
    "H1 = Dense(300, name=\"Dense1\")(X)\n",
    "Z1 = Activation(\"relu\", name=\"Activation1\")(H1)\n",
    "H2 = Dense(50, name=\"Dense2\")(Z1)\n",
    "Z2 = Activation(\"relu\", name=\"Activation2\")(H2)\n",
    "Y = Dense(12, activation=\"softmax\")(Z2)\n",
    "\n",
    "classifier3 = Model(X, Y)\n",
    "classifier3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2291 samples, validate on 573 samples\n",
      "Epoch 1/150\n",
      "2291/2291 [==============================] - 1s 323us/sample - loss: 2.2569 - acc: 0.2575 - val_loss: 2.0783 - val_acc: 0.2949\n",
      "Epoch 2/150\n",
      "2291/2291 [==============================] - 0s 162us/sample - loss: 2.1192 - acc: 0.2938 - val_loss: 2.0954 - val_acc: 0.1832\n",
      "Epoch 3/150\n",
      "2291/2291 [==============================] - 0s 167us/sample - loss: 2.0985 - acc: 0.2837 - val_loss: 2.0760 - val_acc: 0.2565\n",
      "Epoch 4/150\n",
      "2291/2291 [==============================] - 0s 166us/sample - loss: 2.0855 - acc: 0.2994 - val_loss: 2.0443 - val_acc: 0.3490\n",
      "Epoch 5/150\n",
      "2291/2291 [==============================] - 0s 169us/sample - loss: 2.0372 - acc: 0.3125 - val_loss: 1.9924 - val_acc: 0.3421\n",
      "Epoch 6/150\n",
      "2291/2291 [==============================] - 0s 168us/sample - loss: 1.9974 - acc: 0.3208 - val_loss: 2.0165 - val_acc: 0.3176\n",
      "Epoch 7/150\n",
      "2291/2291 [==============================] - 0s 170us/sample - loss: 1.9764 - acc: 0.3151 - val_loss: 1.9623 - val_acc: 0.3403\n",
      "Epoch 8/150\n",
      "2291/2291 [==============================] - 0s 167us/sample - loss: 1.9954 - acc: 0.2946 - val_loss: 2.0619 - val_acc: 0.3368\n",
      "Epoch 9/150\n",
      "2291/2291 [==============================] - 0s 164us/sample - loss: 1.9970 - acc: 0.2876 - val_loss: 1.9963 - val_acc: 0.3351\n",
      "Epoch 10/150\n",
      "2291/2291 [==============================] - 0s 168us/sample - loss: 1.9678 - acc: 0.3221 - val_loss: 1.9421 - val_acc: 0.3473\n",
      "Epoch 11/150\n",
      "2291/2291 [==============================] - 0s 170us/sample - loss: 1.9364 - acc: 0.3217 - val_loss: 1.9900 - val_acc: 0.3316\n",
      "Epoch 12/150\n",
      "2291/2291 [==============================] - 0s 166us/sample - loss: 1.9466 - acc: 0.3217 - val_loss: 1.9071 - val_acc: 0.3508\n",
      "Epoch 13/150\n",
      "2291/2291 [==============================] - 0s 170us/sample - loss: 1.9141 - acc: 0.3221 - val_loss: 1.9207 - val_acc: 0.3421\n",
      "Epoch 14/150\n",
      "2291/2291 [==============================] - 0s 167us/sample - loss: 1.9124 - acc: 0.3191 - val_loss: 1.9876 - val_acc: 0.2688\n",
      "Epoch 15/150\n",
      "2291/2291 [==============================] - 0s 168us/sample - loss: 1.9099 - acc: 0.3256 - val_loss: 1.8974 - val_acc: 0.3403\n",
      "Epoch 16/150\n",
      "2291/2291 [==============================] - 0s 167us/sample - loss: 1.9099 - acc: 0.3204 - val_loss: 1.9121 - val_acc: 0.3316\n",
      "Epoch 17/150\n",
      "2291/2291 [==============================] - 0s 163us/sample - loss: 1.8882 - acc: 0.3317 - val_loss: 1.8593 - val_acc: 0.3543\n",
      "Epoch 18/150\n",
      "2291/2291 [==============================] - 0s 162us/sample - loss: 1.9189 - acc: 0.3239 - val_loss: 1.8596 - val_acc: 0.3490\n",
      "Epoch 19/150\n",
      "2291/2291 [==============================] - 0s 171us/sample - loss: 1.8942 - acc: 0.3326 - val_loss: 1.8528 - val_acc: 0.3508\n",
      "Epoch 20/150\n",
      "2291/2291 [==============================] - 0s 168us/sample - loss: 1.8680 - acc: 0.3313 - val_loss: 1.8927 - val_acc: 0.3473\n",
      "Epoch 21/150\n",
      "2291/2291 [==============================] - 0s 174us/sample - loss: 1.8992 - acc: 0.3304 - val_loss: 1.8839 - val_acc: 0.3403\n",
      "Epoch 22/150\n",
      "2291/2291 [==============================] - 0s 169us/sample - loss: 1.8755 - acc: 0.3304 - val_loss: 1.8342 - val_acc: 0.3508\n",
      "Epoch 23/150\n",
      "2291/2291 [==============================] - 0s 159us/sample - loss: 1.8677 - acc: 0.3269 - val_loss: 1.8463 - val_acc: 0.3508\n",
      "Epoch 24/150\n",
      "2291/2291 [==============================] - 0s 168us/sample - loss: 1.8564 - acc: 0.3322 - val_loss: 1.8357 - val_acc: 0.3508\n",
      "Epoch 25/150\n",
      "2291/2291 [==============================] - 0s 170us/sample - loss: 1.8568 - acc: 0.3326 - val_loss: 1.8375 - val_acc: 0.3490\n",
      "Epoch 26/150\n",
      "2291/2291 [==============================] - 0s 167us/sample - loss: 1.8332 - acc: 0.3339 - val_loss: 1.8250 - val_acc: 0.3508\n",
      "Epoch 27/150\n",
      "2291/2291 [==============================] - 0s 166us/sample - loss: 1.8434 - acc: 0.3304 - val_loss: 1.8251 - val_acc: 0.3490\n",
      "Epoch 28/150\n",
      "2291/2291 [==============================] - 0s 170us/sample - loss: 1.8386 - acc: 0.3352 - val_loss: 1.8243 - val_acc: 0.3438\n",
      "Epoch 29/150\n",
      "2291/2291 [==============================] - 0s 169us/sample - loss: 1.8482 - acc: 0.3300 - val_loss: 1.8280 - val_acc: 0.3525\n",
      "Epoch 30/150\n",
      "2291/2291 [==============================] - 0s 167us/sample - loss: 1.8284 - acc: 0.3344 - val_loss: 1.8210 - val_acc: 0.3473\n",
      "Epoch 31/150\n",
      "2291/2291 [==============================] - 0s 170us/sample - loss: 1.8380 - acc: 0.3274 - val_loss: 1.8210 - val_acc: 0.3490\n",
      "Epoch 32/150\n",
      "2291/2291 [==============================] - 0s 164us/sample - loss: 1.8422 - acc: 0.3278 - val_loss: 1.8149 - val_acc: 0.3473\n",
      "Epoch 33/150\n",
      "2291/2291 [==============================] - 0s 169us/sample - loss: 1.8280 - acc: 0.3304 - val_loss: 1.8658 - val_acc: 0.3264\n",
      "Epoch 34/150\n",
      "2291/2291 [==============================] - 0s 173us/sample - loss: 1.8402 - acc: 0.3309 - val_loss: 1.8692 - val_acc: 0.3473\n",
      "Epoch 35/150\n",
      "2291/2291 [==============================] - 0s 167us/sample - loss: 1.8564 - acc: 0.3199 - val_loss: 1.8095 - val_acc: 0.3508\n",
      "Epoch 36/150\n",
      "2291/2291 [==============================] - 0s 170us/sample - loss: 1.8178 - acc: 0.3317 - val_loss: 1.8255 - val_acc: 0.3473\n",
      "Epoch 37/150\n",
      "2291/2291 [==============================] - 0s 172us/sample - loss: 1.8281 - acc: 0.3352 - val_loss: 1.8204 - val_acc: 0.3525\n",
      "Epoch 38/150\n",
      "2291/2291 [==============================] - 0s 171us/sample - loss: 1.8254 - acc: 0.3344 - val_loss: 1.8345 - val_acc: 0.3403\n",
      "Epoch 39/150\n",
      "2291/2291 [==============================] - 0s 165us/sample - loss: 1.8244 - acc: 0.3335 - val_loss: 1.8163 - val_acc: 0.3490\n",
      "Epoch 40/150\n",
      "2291/2291 [==============================] - 0s 169us/sample - loss: 1.8075 - acc: 0.3370 - val_loss: 1.8088 - val_acc: 0.3508\n",
      "Epoch 41/150\n",
      "2291/2291 [==============================] - 0s 166us/sample - loss: 1.8143 - acc: 0.3269 - val_loss: 1.8505 - val_acc: 0.3438\n",
      "Epoch 42/150\n",
      "2291/2291 [==============================] - 0s 165us/sample - loss: 1.8182 - acc: 0.3234 - val_loss: 1.8023 - val_acc: 0.3525\n",
      "Epoch 43/150\n",
      "2291/2291 [==============================] - 0s 166us/sample - loss: 1.8043 - acc: 0.3357 - val_loss: 1.8319 - val_acc: 0.3490\n",
      "Epoch 44/150\n",
      "2291/2291 [==============================] - 0s 170us/sample - loss: 1.8120 - acc: 0.3300 - val_loss: 1.8186 - val_acc: 0.3438\n",
      "Epoch 45/150\n",
      "2291/2291 [==============================] - 0s 169us/sample - loss: 1.8072 - acc: 0.3322 - val_loss: 1.8220 - val_acc: 0.3438\n",
      "Epoch 46/150\n",
      "2291/2291 [==============================] - 0s 170us/sample - loss: 1.8075 - acc: 0.3339 - val_loss: 1.8335 - val_acc: 0.3316\n",
      "Epoch 47/150\n",
      "2291/2291 [==============================] - 0s 173us/sample - loss: 1.8092 - acc: 0.3247 - val_loss: 1.8134 - val_acc: 0.3508\n",
      "Epoch 48/150\n",
      "2291/2291 [==============================] - 0s 170us/sample - loss: 1.8032 - acc: 0.3291 - val_loss: 1.8588 - val_acc: 0.3403\n",
      "Epoch 49/150\n",
      "2291/2291 [==============================] - 0s 170us/sample - loss: 1.8108 - acc: 0.3317 - val_loss: 1.8175 - val_acc: 0.3455\n",
      "Epoch 50/150\n",
      "2291/2291 [==============================] - 0s 169us/sample - loss: 1.8013 - acc: 0.3326 - val_loss: 1.8727 - val_acc: 0.3421\n",
      "Epoch 51/150\n",
      "2291/2291 [==============================] - 0s 169us/sample - loss: 1.8221 - acc: 0.3282 - val_loss: 1.8054 - val_acc: 0.3490\n",
      "Epoch 52/150\n",
      "2291/2291 [==============================] - 0s 170us/sample - loss: 1.8005 - acc: 0.3335 - val_loss: 1.7944 - val_acc: 0.3578\n",
      "Epoch 53/150\n",
      "2291/2291 [==============================] - 0s 168us/sample - loss: 1.7888 - acc: 0.3387 - val_loss: 1.7875 - val_acc: 0.3560\n",
      "Epoch 54/150\n",
      "2291/2291 [==============================] - 0s 170us/sample - loss: 1.8169 - acc: 0.3418 - val_loss: 1.8891 - val_acc: 0.3351\n",
      "Epoch 55/150\n",
      "2291/2291 [==============================] - 0s 167us/sample - loss: 1.8099 - acc: 0.3361 - val_loss: 1.8089 - val_acc: 0.3560\n",
      "Epoch 56/150\n",
      "2291/2291 [==============================] - 0s 170us/sample - loss: 1.8112 - acc: 0.3300 - val_loss: 1.8229 - val_acc: 0.3403\n",
      "Epoch 57/150\n",
      "2291/2291 [==============================] - 0s 168us/sample - loss: 1.8070 - acc: 0.3361 - val_loss: 1.7940 - val_acc: 0.3508\n",
      "Epoch 58/150\n",
      "2291/2291 [==============================] - 0s 169us/sample - loss: 1.7896 - acc: 0.3326 - val_loss: 1.8192 - val_acc: 0.3508\n",
      "Epoch 59/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2291/2291 [==============================] - 0s 169us/sample - loss: 1.8048 - acc: 0.3309 - val_loss: 1.8067 - val_acc: 0.3508\n",
      "Epoch 60/150\n",
      "2291/2291 [==============================] - 0s 167us/sample - loss: 1.7861 - acc: 0.3409 - val_loss: 1.7840 - val_acc: 0.3578\n",
      "Epoch 61/150\n",
      "2291/2291 [==============================] - 0s 175us/sample - loss: 1.7951 - acc: 0.3396 - val_loss: 1.8165 - val_acc: 0.3421\n",
      "Epoch 62/150\n",
      "2291/2291 [==============================] - 0s 168us/sample - loss: 1.7884 - acc: 0.3387 - val_loss: 1.7880 - val_acc: 0.3525\n",
      "Epoch 63/150\n",
      "2291/2291 [==============================] - 0s 171us/sample - loss: 1.8004 - acc: 0.3352 - val_loss: 1.7936 - val_acc: 0.3525\n",
      "Epoch 64/150\n",
      "2291/2291 [==============================] - 0s 170us/sample - loss: 1.7873 - acc: 0.3317 - val_loss: 1.7919 - val_acc: 0.3578\n",
      "Epoch 65/150\n",
      "2291/2291 [==============================] - 0s 167us/sample - loss: 1.7855 - acc: 0.3387 - val_loss: 1.7917 - val_acc: 0.3525\n",
      "Epoch 66/150\n",
      "2291/2291 [==============================] - 0s 167us/sample - loss: 1.8007 - acc: 0.3357 - val_loss: 1.7880 - val_acc: 0.3647\n",
      "Epoch 67/150\n",
      "2291/2291 [==============================] - 0s 166us/sample - loss: 1.7860 - acc: 0.3348 - val_loss: 1.8326 - val_acc: 0.3386\n",
      "Epoch 68/150\n",
      "2291/2291 [==============================] - 0s 170us/sample - loss: 1.7999 - acc: 0.3330 - val_loss: 1.8295 - val_acc: 0.3473\n",
      "Epoch 69/150\n",
      "2291/2291 [==============================] - 0s 172us/sample - loss: 1.7920 - acc: 0.3378 - val_loss: 1.7904 - val_acc: 0.3543\n",
      "Epoch 70/150\n",
      "2291/2291 [==============================] - 0s 168us/sample - loss: 1.8031 - acc: 0.3335 - val_loss: 1.7922 - val_acc: 0.3560\n",
      "Epoch 71/150\n",
      "2291/2291 [==============================] - 0s 168us/sample - loss: 1.7892 - acc: 0.3387 - val_loss: 1.8014 - val_acc: 0.3421\n",
      "Epoch 72/150\n",
      "2291/2291 [==============================] - 0s 166us/sample - loss: 1.7903 - acc: 0.3352 - val_loss: 1.8125 - val_acc: 0.3473\n",
      "Epoch 73/150\n",
      "2291/2291 [==============================] - 0s 172us/sample - loss: 1.7941 - acc: 0.3396 - val_loss: 1.8118 - val_acc: 0.3386\n",
      "Epoch 74/150\n",
      "2291/2291 [==============================] - 0s 169us/sample - loss: 1.7892 - acc: 0.3405 - val_loss: 1.7817 - val_acc: 0.3543\n",
      "Epoch 75/150\n",
      "2291/2291 [==============================] - 0s 168us/sample - loss: 1.7777 - acc: 0.3352 - val_loss: 1.8170 - val_acc: 0.3386\n",
      "Epoch 76/150\n",
      "2291/2291 [==============================] - 0s 169us/sample - loss: 1.7876 - acc: 0.3317 - val_loss: 1.7865 - val_acc: 0.3386\n",
      "Epoch 77/150\n",
      "2291/2291 [==============================] - 0s 172us/sample - loss: 1.7787 - acc: 0.3387 - val_loss: 1.7780 - val_acc: 0.3525\n",
      "Epoch 78/150\n",
      "2291/2291 [==============================] - 0s 169us/sample - loss: 1.7861 - acc: 0.3383 - val_loss: 1.8025 - val_acc: 0.3490\n",
      "Epoch 79/150\n",
      "2291/2291 [==============================] - 0s 172us/sample - loss: 1.7908 - acc: 0.3378 - val_loss: 1.8862 - val_acc: 0.3490\n",
      "Epoch 80/150\n",
      "2291/2291 [==============================] - 0s 164us/sample - loss: 1.7788 - acc: 0.3400 - val_loss: 1.7997 - val_acc: 0.3386\n",
      "Epoch 81/150\n",
      "2291/2291 [==============================] - 0s 165us/sample - loss: 1.7940 - acc: 0.3378 - val_loss: 1.8169 - val_acc: 0.3159\n",
      "Epoch 82/150\n",
      "2291/2291 [==============================] - 0s 170us/sample - loss: 1.7729 - acc: 0.3405 - val_loss: 1.7980 - val_acc: 0.3386\n",
      "Epoch 83/150\n",
      "2291/2291 [==============================] - 0s 168us/sample - loss: 1.7938 - acc: 0.3300 - val_loss: 1.7905 - val_acc: 0.3630\n",
      "Epoch 84/150\n",
      "2291/2291 [==============================] - 0s 172us/sample - loss: 1.7834 - acc: 0.3287 - val_loss: 1.7949 - val_acc: 0.3543\n",
      "Epoch 85/150\n",
      "2291/2291 [==============================] - 0s 166us/sample - loss: 1.7770 - acc: 0.3405 - val_loss: 1.8218 - val_acc: 0.3473\n",
      "Epoch 86/150\n",
      "2291/2291 [==============================] - 0s 164us/sample - loss: 1.7744 - acc: 0.3418 - val_loss: 1.7960 - val_acc: 0.3403\n",
      "Epoch 87/150\n",
      "2291/2291 [==============================] - 0s 167us/sample - loss: 1.7648 - acc: 0.3448 - val_loss: 1.7941 - val_acc: 0.3560\n",
      "Epoch 88/150\n",
      "2291/2291 [==============================] - 0s 170us/sample - loss: 1.8042 - acc: 0.3365 - val_loss: 1.7672 - val_acc: 0.3578\n",
      "Epoch 89/150\n",
      "2291/2291 [==============================] - 0s 173us/sample - loss: 1.7657 - acc: 0.3431 - val_loss: 1.7744 - val_acc: 0.3508\n",
      "Epoch 90/150\n",
      "2291/2291 [==============================] - 0s 168us/sample - loss: 1.7642 - acc: 0.3422 - val_loss: 1.7878 - val_acc: 0.3560\n",
      "Epoch 91/150\n",
      "2291/2291 [==============================] - 0s 163us/sample - loss: 1.7638 - acc: 0.3405 - val_loss: 1.7883 - val_acc: 0.3578\n",
      "Epoch 92/150\n",
      "2291/2291 [==============================] - 0s 166us/sample - loss: 1.7724 - acc: 0.3426 - val_loss: 1.7774 - val_acc: 0.3490\n",
      "Epoch 93/150\n",
      "2291/2291 [==============================] - 0s 169us/sample - loss: 1.7815 - acc: 0.3409 - val_loss: 1.8409 - val_acc: 0.3473\n",
      "Epoch 94/150\n",
      "2291/2291 [==============================] - 0s 163us/sample - loss: 1.7851 - acc: 0.3396 - val_loss: 1.8072 - val_acc: 0.3386\n",
      "Epoch 95/150\n",
      "2291/2291 [==============================] - 0s 168us/sample - loss: 1.7946 - acc: 0.3409 - val_loss: 1.7846 - val_acc: 0.3560\n",
      "Epoch 96/150\n",
      "2291/2291 [==============================] - 0s 168us/sample - loss: 1.7730 - acc: 0.3396 - val_loss: 1.7705 - val_acc: 0.3508\n",
      "Epoch 97/150\n",
      "2291/2291 [==============================] - 0s 165us/sample - loss: 1.7592 - acc: 0.3400 - val_loss: 1.8010 - val_acc: 0.3473\n",
      "Epoch 98/150\n",
      "2291/2291 [==============================] - 0s 172us/sample - loss: 1.7945 - acc: 0.3370 - val_loss: 1.8231 - val_acc: 0.3159\n",
      "Epoch 99/150\n",
      "2291/2291 [==============================] - 0s 170us/sample - loss: 1.7802 - acc: 0.3396 - val_loss: 1.7857 - val_acc: 0.3403\n",
      "Epoch 100/150\n",
      "2291/2291 [==============================] - 0s 164us/sample - loss: 1.7661 - acc: 0.3422 - val_loss: 1.8035 - val_acc: 0.3543\n",
      "Epoch 101/150\n",
      "2291/2291 [==============================] - 0s 168us/sample - loss: 1.7809 - acc: 0.3409 - val_loss: 1.8419 - val_acc: 0.3560\n",
      "Epoch 102/150\n",
      "2291/2291 [==============================] - 0s 163us/sample - loss: 1.7823 - acc: 0.3392 - val_loss: 1.7655 - val_acc: 0.3473\n",
      "Epoch 103/150\n",
      "2291/2291 [==============================] - 0s 169us/sample - loss: 1.7630 - acc: 0.3448 - val_loss: 1.7747 - val_acc: 0.3508\n",
      "Epoch 104/150\n",
      "2291/2291 [==============================] - 0s 164us/sample - loss: 1.7604 - acc: 0.3405 - val_loss: 1.7727 - val_acc: 0.3578\n",
      "Epoch 105/150\n",
      "2291/2291 [==============================] - 0s 170us/sample - loss: 1.7575 - acc: 0.3453 - val_loss: 1.7865 - val_acc: 0.3386\n",
      "Epoch 106/150\n",
      "2291/2291 [==============================] - 0s 165us/sample - loss: 1.7759 - acc: 0.3418 - val_loss: 1.7847 - val_acc: 0.3438\n",
      "Epoch 107/150\n",
      "2291/2291 [==============================] - 0s 170us/sample - loss: 1.7610 - acc: 0.3418 - val_loss: 1.7709 - val_acc: 0.3543\n",
      "Epoch 108/150\n",
      "2291/2291 [==============================] - 0s 170us/sample - loss: 1.7655 - acc: 0.3409 - val_loss: 1.7831 - val_acc: 0.3578\n",
      "Epoch 109/150\n",
      "2291/2291 [==============================] - 0s 167us/sample - loss: 1.7570 - acc: 0.3488 - val_loss: 1.7709 - val_acc: 0.3543\n",
      "Epoch 110/150\n",
      "2291/2291 [==============================] - 0s 170us/sample - loss: 1.7664 - acc: 0.3457 - val_loss: 1.7760 - val_acc: 0.3525\n",
      "Epoch 111/150\n",
      "2291/2291 [==============================] - 0s 170us/sample - loss: 1.7765 - acc: 0.3400 - val_loss: 1.7671 - val_acc: 0.3525\n",
      "Epoch 112/150\n",
      "2291/2291 [==============================] - 0s 171us/sample - loss: 1.7597 - acc: 0.3488 - val_loss: 1.7694 - val_acc: 0.3613\n",
      "Epoch 113/150\n",
      "2291/2291 [==============================] - 0s 171us/sample - loss: 1.7582 - acc: 0.3505 - val_loss: 1.7667 - val_acc: 0.3421\n",
      "Epoch 114/150\n",
      "2291/2291 [==============================] - 0s 164us/sample - loss: 1.7580 - acc: 0.3448 - val_loss: 1.8134 - val_acc: 0.3455\n",
      "Epoch 115/150\n",
      "2291/2291 [==============================] - 0s 168us/sample - loss: 1.7727 - acc: 0.3418 - val_loss: 1.7797 - val_acc: 0.3543\n",
      "Epoch 116/150\n",
      "2291/2291 [==============================] - 0s 169us/sample - loss: 1.7733 - acc: 0.3470 - val_loss: 1.8032 - val_acc: 0.3473\n",
      "Epoch 117/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2291/2291 [==============================] - 0s 167us/sample - loss: 1.7708 - acc: 0.3348 - val_loss: 1.7798 - val_acc: 0.3438\n",
      "Epoch 118/150\n",
      "2291/2291 [==============================] - 0s 173us/sample - loss: 1.7706 - acc: 0.3474 - val_loss: 1.8232 - val_acc: 0.3386\n",
      "Epoch 119/150\n",
      "2291/2291 [==============================] - 0s 171us/sample - loss: 1.7666 - acc: 0.3361 - val_loss: 1.7684 - val_acc: 0.3578\n",
      "Epoch 120/150\n",
      "2291/2291 [==============================] - 0s 169us/sample - loss: 1.7519 - acc: 0.3474 - val_loss: 1.8166 - val_acc: 0.3525\n",
      "Epoch 121/150\n",
      "2291/2291 [==============================] - 0s 169us/sample - loss: 1.7705 - acc: 0.3518 - val_loss: 1.7712 - val_acc: 0.3595\n",
      "Epoch 122/150\n",
      "2291/2291 [==============================] - 0s 165us/sample - loss: 1.7773 - acc: 0.3357 - val_loss: 1.7718 - val_acc: 0.3455\n",
      "Epoch 123/150\n",
      "2291/2291 [==============================] - 0s 170us/sample - loss: 1.7556 - acc: 0.3457 - val_loss: 1.7791 - val_acc: 0.3647\n",
      "Epoch 124/150\n",
      "2291/2291 [==============================] - 0s 167us/sample - loss: 1.7573 - acc: 0.3435 - val_loss: 1.7750 - val_acc: 0.3560\n",
      "Epoch 125/150\n",
      "2291/2291 [==============================] - 0s 166us/sample - loss: 1.7722 - acc: 0.3435 - val_loss: 1.7987 - val_acc: 0.3455\n",
      "Epoch 126/150\n",
      "2291/2291 [==============================] - 0s 165us/sample - loss: 1.7582 - acc: 0.3488 - val_loss: 1.7640 - val_acc: 0.3560\n",
      "Epoch 127/150\n",
      "2291/2291 [==============================] - 0s 166us/sample - loss: 1.7580 - acc: 0.3466 - val_loss: 1.7770 - val_acc: 0.3455\n",
      "Epoch 128/150\n",
      "2291/2291 [==============================] - 0s 163us/sample - loss: 1.7519 - acc: 0.3466 - val_loss: 1.7935 - val_acc: 0.3525\n",
      "Epoch 129/150\n",
      "2291/2291 [==============================] - 0s 170us/sample - loss: 1.7591 - acc: 0.3474 - val_loss: 1.7653 - val_acc: 0.3630\n",
      "Epoch 130/150\n",
      "2291/2291 [==============================] - 0s 167us/sample - loss: 1.7513 - acc: 0.3492 - val_loss: 1.7698 - val_acc: 0.3560\n",
      "Epoch 131/150\n",
      "2291/2291 [==============================] - 0s 171us/sample - loss: 1.7529 - acc: 0.3466 - val_loss: 1.7651 - val_acc: 0.3613\n",
      "Epoch 132/150\n",
      "2291/2291 [==============================] - 0s 171us/sample - loss: 1.7589 - acc: 0.3426 - val_loss: 1.7694 - val_acc: 0.3595\n",
      "Epoch 133/150\n",
      "2291/2291 [==============================] - 0s 172us/sample - loss: 1.7538 - acc: 0.3435 - val_loss: 1.8081 - val_acc: 0.3595\n",
      "Epoch 134/150\n",
      "2291/2291 [==============================] - 0s 168us/sample - loss: 1.7454 - acc: 0.3431 - val_loss: 1.8270 - val_acc: 0.3508\n",
      "Epoch 135/150\n",
      "2291/2291 [==============================] - 0s 166us/sample - loss: 1.7498 - acc: 0.3448 - val_loss: 1.7700 - val_acc: 0.3578\n",
      "Epoch 136/150\n",
      "2291/2291 [==============================] - 0s 169us/sample - loss: 1.7571 - acc: 0.3352 - val_loss: 1.8166 - val_acc: 0.3578\n",
      "Epoch 137/150\n",
      "2291/2291 [==============================] - 0s 165us/sample - loss: 1.7605 - acc: 0.3413 - val_loss: 1.8128 - val_acc: 0.3543\n",
      "Epoch 138/150\n",
      "2291/2291 [==============================] - 0s 168us/sample - loss: 1.7553 - acc: 0.3431 - val_loss: 1.7987 - val_acc: 0.3473\n",
      "Epoch 139/150\n",
      "2291/2291 [==============================] - 0s 174us/sample - loss: 1.7729 - acc: 0.3448 - val_loss: 1.8111 - val_acc: 0.3525\n",
      "Epoch 140/150\n",
      "2291/2291 [==============================] - 0s 165us/sample - loss: 1.7499 - acc: 0.3488 - val_loss: 1.7843 - val_acc: 0.3525\n",
      "Epoch 141/150\n",
      "2291/2291 [==============================] - 0s 168us/sample - loss: 1.7628 - acc: 0.3457 - val_loss: 1.7705 - val_acc: 0.3543\n",
      "Epoch 142/150\n",
      "2291/2291 [==============================] - 0s 164us/sample - loss: 1.7469 - acc: 0.3492 - val_loss: 1.7836 - val_acc: 0.3543\n",
      "Epoch 143/150\n",
      "2291/2291 [==============================] - 0s 170us/sample - loss: 1.7451 - acc: 0.3470 - val_loss: 1.7689 - val_acc: 0.3455\n",
      "Epoch 144/150\n",
      "2291/2291 [==============================] - 0s 168us/sample - loss: 1.7513 - acc: 0.3435 - val_loss: 1.7638 - val_acc: 0.3578\n",
      "Epoch 145/150\n",
      "2291/2291 [==============================] - 0s 171us/sample - loss: 1.7518 - acc: 0.3501 - val_loss: 1.7671 - val_acc: 0.3647\n",
      "Epoch 146/150\n",
      "2291/2291 [==============================] - 0s 169us/sample - loss: 1.7485 - acc: 0.3488 - val_loss: 1.7884 - val_acc: 0.3508\n",
      "Epoch 147/150\n",
      "2291/2291 [==============================] - 0s 169us/sample - loss: 1.7581 - acc: 0.3453 - val_loss: 1.7919 - val_acc: 0.3403\n",
      "Epoch 148/150\n",
      "2291/2291 [==============================] - 0s 166us/sample - loss: 1.7623 - acc: 0.3413 - val_loss: 1.7774 - val_acc: 0.3630\n",
      "Epoch 149/150\n",
      "2291/2291 [==============================] - 0s 169us/sample - loss: 1.7595 - acc: 0.3422 - val_loss: 1.7663 - val_acc: 0.3578\n",
      "Epoch 150/150\n",
      "2291/2291 [==============================] - 0s 165us/sample - loss: 1.7656 - acc: 0.3387 - val_loss: 1.7654 - val_acc: 0.3543\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f765b125910>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_test = data_transformed[:500]\n",
    "trans_train = data_transformed[500:]\n",
    "\n",
    "xtrain = np.array([np.array(x) for (x,y) in trans_train])\n",
    "ytrain = np.array([y for (x,y) in trans_train])\n",
    "classifier3.compile(optimizer=Adam(lr=0.0003), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "classifier3.fit(xtrain, ytrain, batch_size=32, epochs=150, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
